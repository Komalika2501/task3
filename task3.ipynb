{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPS+z/nPextSE4XVh6vy7DM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Komalika2501/task3/blob/main/task3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Implement two hidden layers neural network classifier from scratch in JAX (task - 3)*"
      ],
      "metadata": {
        "id": "m_XBU_e0WaHg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBLRZBE7jXGo"
      },
      "outputs": [],
      "source": [
        "# todo: define init MLP and add the predict\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.scipy.special import logsumexp\n",
        "import numpy as np\n",
        "from jax import jit, vmap, pmap, grad, value_and_grad\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 0\n",
        "mnist_img_size = (28,28)\n",
        "def init_MLP(layer_widths,parent_key, scale=0.01):\n",
        "\n",
        "  params = []\n",
        "  keys = jax.random.split(parent_key, num=len(layer_widths)-1)\n",
        "\n",
        "  for in_width, out_width, key in zip(layer_widths[:-1], layer_widths[1:], keys):\n",
        "    weight_key, bias_key = jax.random.split(key)\n",
        "    params.append(\n",
        "        [scale*jax.random.normal(weight_key, shape=(out_width, in_width)), \n",
        "         scale*jax.random.normal(bias_key, shape=(out_width,))]\n",
        "                  )\n",
        "  return params\n",
        "\n",
        "key = jax.random.PRNGKey(seed)\n",
        "MLP_params = init_MLP([784,512,256,10],key)\n",
        "\n",
        "print(jax.tree_map(lambda x:x.shape,MLP_params))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNqhiUTzjlGB",
        "outputId": "8b13f81a-c94e-484e-a491-0b54fd4e895e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[(512, 784), (512,)], [(256, 512), (256,)], [(10, 256), (10,)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def MLP_predict(params, x):\n",
        "  # computing the forward pass for each example individually\n",
        "  hidden_layers = params[:-1]\n",
        "\n",
        "  activation = x\n",
        "  # loop over the Relu hidden layers\n",
        "  for w, b in hidden_layers:\n",
        "    activation = jax.nn.relu(jnp.dot(w,activation)+b)\n",
        "\n",
        "  w_last, b_last = params[-1]\n",
        "  logits = jnp.dot(w_last, activation)+b_last\n",
        "\n",
        "  return logits - logsumexp(logits)\n",
        "\n",
        "batched_MLP_predict = vmap(MLP_predict, in_axes=(None,0))\n",
        "\n",
        "# small test\n",
        "dummy_imgs_flat = np.random.randn(16, np.prod(mnist_img_size))\n",
        "print(dummy_imgs_flat.shape)\n",
        "predictions = batched_MLP_predict(MLP_params, dummy_imgs_flat)\n",
        "print(predictions.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7kIbphyqwZ4",
        "outputId": "f87dfb17-2c03-4740-ea31-7a29273120c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 784)\n",
            "(16, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# todo: adding data loading in PyTorch\n",
        "def custom_transform(x):\n",
        "  return np.ravel(np.array(x, dtype=np.float32))\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "  transposed_data = list(zip(*batch))\n",
        "\n",
        "  labels = np.array(transposed_data[1])\n",
        "  imgs = np.array(transposed_data[0])\n",
        "  return imgs,labels\n",
        "\n",
        "\n",
        "train_dataset = MNIST(root='train_mnist', train=True, download= True, transform=custom_transform)\n",
        "# testing the transformation\n",
        "print(type(train_dataset))\n",
        "something = train_dataset[0]\n",
        "print(type(something[0]))\n",
        "\n",
        "# as we see we are working with jax neither PyTorch nor tensorflow \n",
        "#then we have to transform the images in dataset into numpy array from this PIL.image\n",
        "\n",
        "test_dataset = MNIST(root='test_mnist', train= False, download= True, transform = custom_transform )\n",
        "batch_size = 128\n",
        "img = train_dataset[0][0]\n",
        "print(img.shape)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, sampler=None,collate_fn= custom_collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size, shuffle=False, collate_fn=custom_collate_fn, drop_last=True)\n",
        "\n",
        "batch_data = next(iter(train_loader))\n",
        "imgs = batch_data[0]\n",
        "lbls = batch_data[1]\n",
        "print(imgs.shape, imgs[0].dtype, lbls.shape, lbls[0].dtype)\n",
        "\n",
        "# optimization - loading the whole dataset into memory\n",
        "train_images = jnp.array(train_dataset.data).reshape(len(train_dataset), -1)\n",
        "train_lbls = jnp.array(train_dataset.targets)\n",
        "\n",
        "test_images = jnp.array(test_dataset.data).reshape(len(test_dataset), -1)\n",
        "test_lbls = jnp.array(test_dataset.targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpnTYCxO4Ego",
        "outputId": "76632a79-b1bf-4a79-f5ff-69426f5fb5de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torchvision.datasets.mnist.MNIST'>\n",
            "<class 'numpy.ndarray'>\n",
            "(784,)\n",
            "(128, 784) float32 (128,) int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# todo: add the training loop, loss function\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "def loss_fn(params, imgs, gt_lbls):\n",
        "    predictions = batched_MLP_predict(params, imgs)\n",
        "\n",
        "    return -jnp.mean(predictions * gt_lbls)\n",
        "\n",
        "def accuracy(params, dataset_imgs, dataset_lbls):\n",
        "    pred_classes = jnp.argmax(batched_MLP_predict(params, dataset_imgs), axis=1)\n",
        "    return jnp.mean(dataset_lbls == pred_classes)\n",
        "\n",
        "@jit\n",
        "def update(params, imgs, gt_lbls, lr=0.01):\n",
        "    loss, grads = value_and_grad(loss_fn)(params, imgs, gt_lbls)\n",
        "\n",
        "    return loss, jax.tree_multimap(lambda p, g: p - lr*g, params, grads)\n",
        "\n",
        "# Create a MLP\n",
        "MLP_params = init_MLP([np.prod(mnist_img_size), 512, 256, len(MNIST.classes)], key)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    for cnt, (imgs, lbls) in enumerate(train_loader):\n",
        "\n",
        "        gt_labels = jax.nn.one_hot(lbls, len(MNIST.classes))\n",
        "        \n",
        "        loss, MLP_params = update(MLP_params, imgs, gt_labels)\n",
        "        \n",
        "        if cnt % 50 == 0:\n",
        "            print(loss)\n",
        "\n",
        "    print(f'Epoch {epoch}, train acc = {accuracy(MLP_params, train_images, train_lbls)} test acc = {accuracy(MLP_params, test_images, test_lbls)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IryXrlAn7XyC",
        "outputId": "4012eba4-6a62-4520-e348-da219bc0d2ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.23810497\n",
            "0.087891\n",
            "0.06055032\n",
            "0.04873803\n",
            "0.044856805\n",
            "0.041558135\n",
            "0.017226944\n",
            "0.051550265\n",
            "0.030981481\n",
            "0.024762424\n",
            "Epoch 0, train acc = 0.9117000102996826 test acc = 0.915399968624115\n",
            "0.02242488\n",
            "0.038204826\n",
            "0.029759724\n",
            "0.028814156\n",
            "0.03476141\n",
            "0.011736705\n",
            "0.027560491\n",
            "0.030366594\n",
            "0.022153376\n",
            "0.030858075\n",
            "Epoch 1, train acc = 0.9347333312034607 test acc = 0.9343000054359436\n",
            "0.013776625\n",
            "0.022813382\n",
            "0.02567752\n",
            "0.015121998\n",
            "0.019742316\n",
            "0.022333434\n",
            "0.02496249\n",
            "0.022899596\n",
            "0.012124732\n",
            "0.017239286\n",
            "Epoch 2, train acc = 0.9462833404541016 test acc = 0.9441999793052673\n",
            "0.023720494\n",
            "0.029169876\n",
            "0.013067083\n",
            "0.0153065575\n",
            "0.022722801\n",
            "0.012793926\n",
            "0.020874484\n",
            "0.031087339\n",
            "0.019954143\n",
            "0.018512445\n",
            "Epoch 3, train acc = 0.954633355140686 test acc = 0.9508999586105347\n",
            "0.024799528\n",
            "0.020295164\n",
            "0.0257946\n",
            "0.018008364\n",
            "0.010205585\n",
            "0.013567696\n",
            "0.013480702\n",
            "0.025155593\n",
            "0.0061104163\n",
            "0.0073105604\n",
            "Epoch 4, train acc = 0.9609000086784363 test acc = 0.9555999636650085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, lbls = next(iter(test_loader))\n",
        "img = imgs[1].reshape(mnist_img_size)\n",
        "gt_lbl = lbls[1]\n",
        "print(img.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pred = jnp.argmax(MLP_predict(MLP_params, np.ravel(img)))\n",
        "print('pred', pred)\n",
        "print('gt', gt_lbl)\n",
        "\n",
        "plt.imshow(img); plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "pw_b-UegJDSq",
        "outputId": "7424582f-1f53-43b8-9f59-b015ac414ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28)\n",
            "pred 2\n",
            "gt 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uHP-ufvZLMQg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}